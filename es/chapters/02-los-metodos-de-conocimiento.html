<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Los métodos de conocimiento - Something New: AIs and Us</title>
    <link rel="stylesheet" href="../../styles.css">
</head>
<body>
    <div class="container">
        <div class="book-header">
            <div>
                <h1>Something New: AIs and Us</h1>
            </div>
            <div class="book-nav">
                <a href="../../" class="lang-switcher">← Home</a>
                <a href="../index.html" class="lang-switcher">← All Chapters</a>
            </div>
        </div>

        <div class="book-content">
            <h2>Los métodos de conocimiento</h2>
        <p>Secuencias trascendentes</p>
        <p>Los ejemplos más sencillos del cambio exponencial, por ejemplo la duplicación de una cantidad en un determinado período de tiempo si el punto de partida es la unidad de 1, pueden parecer bastante inofensivos o incluso decepcionantes al principio.</p>
        <p>1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096,… es una secuencia familiar incluso para quien no haya mostrado un especial interés por los números. El traqueteo de la secuencia en la cabeza era un ejercicio inofensivo propio de la infancia.</p>
        <p>Existe una secuencia correspondiente, antes de la unidad, que podemos considerar potencialmente sencilla incluso en mayor grado: 0,01, 0,02, 0,04, 0,08, 0,16, 0,32, 0,64, y por último, 1,28. La curiosa sorpresa de esta secuencia, nada mágica por cierto, es lo que le precedía: 0,00015625, 0,0003125, 0,00625, 0,00125, 0,0025, 0,005, 0,01.</p>
        <p>¿De dónde proviene el interés por estas tres secuencias y por qué son representativas de la naturaleza del cambio exponencial?</p>
        <p>Imagine que usted está mirando el mundo que le rodea, y en su intento por descifrarlo y predecir la consecuencia de un determinado fenómeno, usted recoge datos sobre el mismo. Esta recopilación no será tan nítida y clara como las secuencias anteriores. El griterío está asegurado. Errores de medición, errores cometidos durante el proceso o la planificación de las mediciones, otros fenómenos que se inmiscuirán y confundirán sus intentos por alcanzar una clara comprensión, etc.</p>
        <p>El griterío de un entorno natural donde, incluso antes de entrenar sus oídos para ello, usted percibe un modelo que posiblemente sea nuevo, algo que nadie hasta ahora intentó escuchar.</p>
        <p>¿Hay señales en el ruido? Es muy probable que mientras usted intenta responder la pregunta, aparezcan otras opiniones a su alrededor. Y por definición discreparán de la suya, no coincidirán o incluso serán opuestas. Si usted está inmerso en un entorno de investigación y compite por conseguir una ayuda económica, o trabaja en la industria y el producto que intenta diseñar o el servicio que promociona entre usuarios legítimamente trastornados por una descomunal oferta de opciones alternativas, en ambos casos se sentirá confundido por la resistencia mostrada a su original teoría sustentada por las señales.  Debe ser categórico en sus opiniones, incluso debe tener fe en lo que quiere demostrar, una convicción irrazonable de que tiene razón aunque todo el mundo le diga que está equivocado. O aun cuando lo que esté buscando no exista o sea imposible.</p>
        <p>Este es el ámbito de la tercera secuencia, que conduce hasta el 0,01 (o 1 % de la unidad). La dimensión en la que incluso los expertos estarán en su contra. Hace falta tener buen ojo y buen oído, en sentido concreto o abstracto, para entender que frente a la presencia de las distracciones de un mundo de por sí ruidoso, hay algo que está fermentando. La duplicación serena, sin que nadie se dé cuenta salvo usted, para después de varias duplicaciones alcanzar el umbral del 1 % partiendo del objetivo de la unidad.</p>
        <p>Una vez haya llegado al 1 %, estos expertos que siguen sin creerle deberían ser ignominiosamente despojados de su etiqueta. Porque a partir de entonces debería quedarle claro, no tan solo a usted sino a todo el mundo que preste una atención incluso fugaz, de que se trata de una mera cuestión de tiempo. En solo siete duplicaciones sucesivas, usted habrá llegado a la unidad.</p>
        <p>El crecimiento exponencial del Proyecto Genoma Humano</p>
        <p>Si convertimos esta descripción de las secuencias abstractas de números en un ejemplo real, podemos ver lo que sucedió en el gigantesco Proyecto Genoma Humano realizado en Estados Unidos. Iniciado en 1985, la duración del proyecto se estableció originalmente en 15 años. Al igual que cualquier proyecto científico, no quedaba muy claro cómo se salvarían todos los obstáculos, ni cuál sería el enfoque agraciado. ¡A los siete años de iniciar el proyecto solo se había alcanzado el 1 % del objetivo! En aquel momento fueron muchos los que exigieron en voz alta la suspensión o incluso el abandono del proyecto: ¡miren, estamos a medio camino y solo se ha alcanzado el 1 % del objetivo! Aquellos que fueron más prudentes, o los expertos en dinámica exponencial, entendieron que todo iba bien. El logro de la duplicación del 1 % de los pares de bases descodificados cada año durante los siete años anteriores significaba que en otros siete años de duplicaciones, el proyecto alcanzaría su objetivo del 100 % de descodificación de la totalidad del genoma humano.</p>
        <p>Son muchos los fenómenos que están sometidos a un crecimiento exponencial: poblaciones y reacciones nucleares en cadena, por nombrar solo un par de ejemplos. Las poblaciones crecen exponencialmente porque, habida cuenta de que, generación tras generación, las parejas tienen de promedio más de dos hijos, el incremento será acumulativo: aquellos descendientes tendrán también más hijos. Las reacciones nucleares en cadena se producen también cuando los materiales fisibles, el uranio por ejemplo, entre otros productos de fisión produce neutrones, que antes de abandonar el volumen de material generan otra descomposición aparte de átomos de uranio, lo que produce otros neutrones, y así sucesivamente.</p>
        <p>Lo más importante para el tema de este libro, el poder de los sistemas informáticos y de la información, también está creciendo exponencialmente, y lo lleva haciendo desde hace más de 50 años. Sin embargo, no hay una ley natural detrás de esta dinámica, ni una necesidad biológica o física. Es un proyecto de ingeniería que acabó recibiendo el nombre de la persona que lo formuló por primera vez, la Ley de Moore.</p>
        <p>Gordon Moore trabajaba en el circuito integrado recién inventado a comienzos de la década de los 60. Estaba metido en un entorno ruidoso, en lo que se refiere a la necesidad de concentrarse en las particularidades de un nuevo fenómeno en presencia de otras muchas personas que trabajaban simultáneamente en ello. Los ordenadores prácticos irrumpieron hace más o menos un par de décadas, y cada vez son más y más potentes, pero a un ritmo que resultó ser bastante lento si lo analizamos linealmente.</p>
        <p>Se intentaron varios planteamientos para conseguir almacenar más información para los cálculos y para ejecutarlos con mayor rapidez. Los tubos al vacío, las memorias de núcleos magnéticos y otros componentes a los que los medios de comunicación de la época bautizaron con el nombre de “cerebros electrónicos”, eran engorrosos, mostraban unos altos índices de fallos y necesitaban veintenas de especialistas que se encargaran de ellos para cerciorarse de su funcionamiento. El coste de los ordenadores era de millones de dólares y, solo los programas nacionales de investigación o las empresas más grandes, podían permitírselos.</p>
        <p>El invento del transistor para ser utilizado como componente básico para el cálculo, abrió las puertas a una producción, montaje y funcionamiento de los ordenadores mucho más fiables y baratos. Los transistores podían ser empaquetados junto con otros componentes y así crear una valiosa unidad de cálculo llamada circuito integrado. No tan solo eso, sino que dada su naturaleza, existía la posibilidad de predecir el desarrollo de unos componentes de próxima generación más pequeños, más rápidos y más baratos que los anteriores.</p>
        <p>Gordon Moore supo advertir cuáles eran las posibilidades actuales de los procesos de producción, así como el incremento de estas posibilidades en pocos años. Basándose solo en un puñado de puntos de datos trazados en un trozo de papel cuadriculado que todavía sobrevive después de 50 años, formuló con valentía la predicción de que el número de transistores que podía alojarse en un determinado circuito integrado se duplicaría cada año. Posteriormente corrigió la predicción a dos años, y finalmente la estableció en 18 meses, que es el valor actualmente aceptado y utilizado.</p>
        <p>Teniendo en cuenta que solo disponía de unos pocos puntos de datos, esta predicción era bastante audaz, por no decir imprudente. Sin embargo, desde la perspectiva actual, parece que esta intrépida ambición es lo que verdaderamente se necesitaba. Porque lo que sucedió es que, espoleado por la curiosidad, el deseo de sobresalir y la competencia económica esencial, se organizaron más y más grupos de ingenieros para crear circuitos integrados más potentes. Junto con todos los sistemas de apoyo necesarios, juntos tejieron una industria entera. Al comienzo, este proceso se vio impulsado por la capacidad individual de estos grupos y por su capacidad de sacarlo al mercado. No obstante, posteriormente, la Ley de Moore se convirtió por sí misma en una fuerza impulsora, una especie de profecía autocumplida, así como un indicador con el cual medir los logros de los diversos grupos.</p>
        <p>Se predijo repetidamente que la Ley de Moore fracasaría en su deseo de seguir manteniéndose en la siguiente generación, y tarde o temprano está obligada a hacerlo con sus formulaciones más estrictas. En términos más generales, con la extensión de sus predicciones al poder de la informática hay razones para creer que todavía es posible mantenerla durante mucho tiempo. Con solo cambiar la silicona por otros sustratos para los circuitos, crear componentes tridimensionales, sustituir las arquitecturas que ven un impedimento en los fenómenos cuánticos para quienes las explotan al máximo… existen muchos criterios para superar las posibles barricadas que se encontrarán para demostrar la oportunidad de esta ley, de la misma manera que se han superado otras en los últimos cincuenta años.</p>
        <p>Cabe señalar que la divulgación del conocimiento es la base de la Ley de Moore. Ningún grupo individual que trabajara en secreto podría esperar ser el único capaz de resolver los problemas que van surgiendo en el camino de la próxima generación de soluciones, o el único que lo intenta. Esto solo será posible con la colaboración de muchos grupos. Basta con que uno de ellos consiga un avance para el descubrimiento de la solución necesaria. Todos los demás lo aprovecharán gracias a unos acuerdos de licencias que incorporan la solución en las plantas de producción de próxima generación para la producción en masa de circuitos integrados, que hoy son producidos en miles de millones al año.</p>
        <p>El complejo ecosistema de enclavamiento de la infraestructura industrial necesario para mantener el ritmo de la evolución en la informática no se reduce a la propia producción de circuitos integrados. También deben evolucionar las herramientas de fabricación para la creación de los circuitos, los sistemas de software que permitan diseñarlos y la ayuda financiera necesaria para invertir en plantas, materias primas, perfeccionamiento, y lo que es más importante… capital humano.</p>
        <p>Independientemente de cuales sean los límites físicos fundamentales para el crecimiento del cálculo, ponderado por la generalizada Ley de Moore, todavía quedan muy lejos. Los avances que hemos vivido en los últimos 50 años de crecimiento del poder de la informática van a verse infinitamente eclipsados por los avances de los próximos 50 años. De hecho, en los dos próximos años van a verse superados por la propia naturaleza del crecimiento exponencial, Y de nuevo a los dos años siguientes, y así sucesivamente.</p>
        <p>El poder de las duplicaciones</p>
        <p>No importa, evidentemente, la rapidez con la que se desarrolla una secuencia exponencial. No es necesario que se duplique en un año para que sea exponencial. Son unidades meramente arbitrarias, y lo hará cualquier cambio acumulativo donde la cantidad resultante aumente en la cantidad expresada en el propio resultado. Si tenemos una cantidad de 100 y ésta aumenta en 10, obtendremos 110, 120, 130, etc. Esto es un crecimiento lineal. Pero si tenemos una cantidad que aumenta un 10 %, entonces la secuencia será 110, 121, 133, y así sucesivamente. Esta pequeña diferencia, que no parece ser demasiado significativa al principio, es lo que prima. Esto es el crecimiento exponencial.</p>
        <p>Hay muchas maneras de expresar este poder, y muy sorprendentes para quienes están acostumbrados a razonar linealmente.</p>
        <p>Miremos por ejemplo la suma de esta secuencia: 1, 2, 4. La suma 7 = 1+2+4 es la cantidad total de la secuencia entera. Y el próximo paso de esta secuencia es 8, cifra superior al total de todos los pasos precedentes. Esto sucede en todos los crecimientos exponenciales. ¡En el próximo período de duplicación del cálculo, en solamente 18 meses, gracias a la Ley de Moore, habrá más transistores y circuitos integrados creados (y ordenadores fabricados con ellos y cálculos realizados gracias a ellos) que en toda la historia de la informática de los últimos cincuenta años o más!</p>
        <p>¿Cuándo es demasiado tarde?</p>
        <p>Otro ejemplo que ilustra el poder de los exponenciales es un sistema cerrado, por ejemplo una charca en la que vive una población de ranas. Si las algas hacen que el lago sea inhabitable para las ranas, y estas cubren cada vez una mayor superficie del lago, desde solo una fracción a una duplicación del uno por ciento cada semana, ¿cuánto tiempo deben vivir las ranas antes de que las algas cubran la mitad de la charca? Por suerte la respuesta es clara: ¡solo una semana ya que en la siguiente duplicación el lago quedará totalmente cubierto por las algas! Lo más alarmante, quizás, es que con solo el 1 % quedan menos de dos meses para que las ranas huyan a otra charca, o busquen la manera de parar la propagación de las algas.</p>
        <p>Nuestra posición privilegiada nos permite ver que lo que sucede en la charca es perjudicial para las ranas. Y esta capacidad para recopilar datos, analizar y prever supone una enorme responsabilidad para comprender si la charca está en buenas condiciones o no. La adopción de medidas para el control de la charca, el recuento de las algas no puede ni será realizado por otros, pero nosotros sí podemos hacerlo.</p>
        <p>Los diversos ejemplos de cambio exponencial que hallamos en la naturaleza se autoalimentan, pero raramente se agrupan en cadenas interactivas que se alimenten recíprocamente. Por otra parte, nuestra civilización tecnológica está llena de cadenas que se retroalimentan y que mantienen la aceleración del cambio exponencial.</p>
        <p>El proyecto de Ray Kurzweil</p>
        <p>El inventor, autor y cofundador de la Universidad de la Singularidad, Ray Kurzweil, lleva décadas recogiendo datos sobre el fenómeno exponencial. De hecho, no basta con reconocer lo que sucede. La explosiva naturaleza de los exponenciales es tal que la elección del momento oportuno es fundamental si uno quiere deslizarse sobre la ola en lugar de quedarse viéndola de lejos.</p>
        <p>Súbase demasiado pronto a la ola, y quienes dicen lo contrario lo tendrán fácil para abatir su entusiasmo o el de sus proveedores de fondos, porque el repunte de la secuencia que sigue el rastro de la hipotética unidad de nuestras secuencias de ejemplo, no se producirá. Súbase demasiado tarde, y ya habrá desplegado todo su poder cuando quiera subirse a ella, lo que hará que el esfuerzo sea costoso, difícil o incluso imposible, mientras los demás habrán coronado ya la cresta.</p>
        <p>Desde los escáneres planos al reconocimiento óptico de caracteres, desde la síntesis musical a la síntesis de voz o los sistemas portátiles para invidentes, todos los inventos de Kurzweil son fruto de un profundo conocimiento de la elección del momento oportuno. Cuando acelerar la investigación y desarrollo para que en el momento en que los sistemas hardware de apoyo alcancen el precio justo y el nivel de integración adecuado, todos los demás componentes del software, la interfaz de usuario, los sistemas de desarrollo y la totalidad del ecosistema de apoyo estén también preparados.</p>
        <p>En el Instituto Santa Fe basado en la investigación de Bela Nagy, hay una base de datos completa de fenómenos exponenciales a la que puede accederse para estudiarla y expandirla aún más.</p>
        <p>Kurzweil reconoció también que con los sistemas recíprocamente interconectados y comunicados de conocimiento humano que no crecen aislados sino que se refuerzan mutuamente, hay exponenciales que se alimentan de exponenciales. A este efecto resultante lo bautizó con el nombre de Ley de Rendimientos Acelerados, que contradice la sabiduría adquirida de la economía clásica en la que se supone que para alcanzar un determinado incremento del rendimiento económico, es necesario que haya una cantidad progresivamente mayor de entrada de capital disponible, la llamada Ley de Rendimientos Decrecientes.</p>
        <p>Al igual que la Ley de Moore, la Ley de Rendimientos Acelerados formulada por Kurzweil es una profecía autocumplida, sostenida por las comunicaciones abiertas y los grupos competidores que aspiran a alcanzar el éxito y la excelencia en sus iniciativas de investigación y de producción industrial. Decididamente pueden infringirse ambas leyes. Si dejamos de creer en la ley de gravitación universal y nos tiramos del quinto piso de un edificio, podríamos hacerlo miles de veces y nunca dejaríamos de caer como una piedra, siendo lo más probable que nos matáramos.  Pero si no hubiéramos intentado hacer unos circuitos mejores, o si hubiéramos decidido que no valía la pena esforzarnos para hacer unos paneles solares mejores, unas mejores baterías, etcétera, siempre y cuando se hubiera detenido también todo el mundo, estos circuitos, paneles y baterías no existirían.</p>
        <p>A fecha de este escrito, Kurzweil es Director de Ingeniería de Google, y según sus propias palabras, el primer trabajo que ha tenido. Gracias al uso de los recursos proporcionados por la empresa, está aplicando sus conocimientos en la creación de una interacción del lenguaje natural con los ordenadores, y en la próxima oleada de interacción del usuario, que hará que los ordenadores sean todavía más fáciles de usar y más capaces de satisfacer nuestras necesidades.</p>
        <p>Conexión de las curvas S</p>
        <p>Una crítica frecuente del análisis y las predicciones de Kurzweil se basa en un malentendido de lo que constituye el exponencial del que habla. Los críticos destacan el hecho de que lo que parece ser un exponencial es en realidad la primera mitad de una curva S o curva logística. Al comienzo parece un exponencial, dada la explotación de las ventajas de una determinada tecnología. No obstante, al cabo de un rato se desestabiliza porque cada vez es más difícil extraer más ventajas de la misma tecnología. Acaba agotándose y falsea la creencia en el poder de la tecnología de quienes predican unos exponenciales interminables.</p>
        <p>Sin duda es verdad: a lo largo de su ciclo, llega un momento en que todas las tecnologías individuales no dan más de sí. A medida que van acercándose a este límite, es inútil querer insistir en exprimirlas más, tanto desde el punto de vista de la ingeniería como de la economía y la rentabilidad. Y esta es la razón por la que nuevos grupos con nuevas ideas intentarán alcanzar el resultado deseado a través de unos enfoques distintos. Los inteligentes captarán el momento en que se agotará la actual generación de tecnologías y trabajarán en paralelo con los grupos punteros del momento para encontrar una nueva tecnología que alcance el objetivo a escala, y mejor que antes. El ciclo de algunas duplicaciones exponenciales se repetirá y se necesitará una tercera generación de soluciones, y así sucesivamente.</p>
        <p>El efecto acumulativo de estas diferentes curvas S, que allanarán las terminaciones de cada una y que se entrelazarán más o menos a la perfección en una cadena de ingenio, innovación y desarrollo industrial, traza el exponencial que apunta Kurzweil en sus análisis.</p>
        <p>Si hablamos de informática, por ejemplo, ha habido un gran número de generaciones de tecnologías informáticas, todas ellas punteras en su momento, que han sido forzadas al máximo y se han visto desbancadas por la siguiente, por otra mejor, más barata y más rápida en la generación del resultado deseado. Década tras década, los relés mecánicos, los tubos al vacío, los transistores y los circuitos integrados han permitido la construcción de los ordenadores más rápidos y más potentes del mundo. Las empresas que los utilizaron fueron líderes en su época y forzaron al máximo las tecnologías, y fueron sustituidas por otros nuevos modelos basados en las nuevas tecnologías en solo unas cuantas duplicaciones del rendimiento.</p>
        <p>Otro ejemplo de que en estos años estamos asistiendo a un cambio radical, lo encontramos en el almacenamiento permanente de la memoria. Las cada vez mayores cantidades de datos que deben ser constantemente registradas por nuestros ordenadores para que, cada vez que se interrumpa el suministro eléctrico y el ordenador se despierte más tarde, podamos recuperar los datos sin necesidad de empezar de nuevo desde cero.  Desde las tarjetas perforadas a la memoria de núcleos magnéticos, la cinta magnética y los discos duros giratorios, ahora que estamos a punto de trasladar el almacenamiento por las necesidades de la próxima generación de un soporte de estado sólido (almacenamiento flash), que va a ser capaz de memorizar órdenes de mucha mayor importancia, y al que se podrá acceder de una manera mucho más rápida, fiable y asequible que con cualquier generación anterior de dispositivos.</p>
        <p>Exponenciales por doquier</p>
        <p>Son muchas las tecnologías que pueden verse a través de la lente de dicha interpretación exponencial de este cambio acelerado. Los períodos de duplicación pueden ser distintos, obviamente, de los 18 meses establecidos por la Ley de Moore en los que solíamos confiar.</p>
        <p>En la energía solar hablamos de la ley de Swanson, que representa el descenso del precio por vatio de un panel fotovoltaico. A partir de 1974, gracias a la creación del primero de estos dispositivos, el coste de más de 70$ por vatio se redujo a 0,30$ por vatio, y el precio continúa bajando. Este descenso es fruto de las economías de escala, de una mejor comprensión de los procesos de fabricación y del nacimiento de un ecosistema de financiación, implementación y mantenimiento de los módulos, así como de los nuevos enfoques básicos de los materiales y de los métodos de construcción, que aumentan considerablemente la eficiencia de un determinado módulo que transforma la luz solar en electricidad.</p>
        <p>Hay una duplicación de la capacidad de almacenamiento de nuestras baterías. Esta duplicación es un período de diez años más sosegado (y exasperante si cree que dedica demasiado tiempo a la carga de sus dispositivos ávidos de potencia). Dependiendo de la metalurgia, la química y los procesos de fabricación, cabe imaginar que en la ilustración de la Ley de Rendimientos Acelerados de Kurzweil, la industria encontraría una manera de acelerar la duplicación mediante la adopción de un enfoque radicalmente nuevo y así lograr la práctica aplicación de lo que antes era imposible.</p>
        <p>Los objetivos establecidos por los propios programas de investigación tales como el Proyecto Genoma Humano, son a menudo ligeramente arbitrarios. Representan un indicador útil pero no el objetivo del desarrollo de los procesos o de su sofisticación, ni por supuesto el deseo de conocimiento y de la posibilidad de adquirirlo de una manera más rápida y económica. Tras la descodificación del genoma humano de un único individuo se esconde la tarea de hacer lo mismo en otros siete mil millones de personas. Detrás del genoma humano está el genoma de otros animales, o de las bacterias de los océanos, o de las bacterias que simbióticamente viven continuamente en todos nosotros y que constituyen nuestro microbioma.</p>
        <p>La capacidad de descodificar el genoma humano no se detuvo a la proporción de uno por tres mil millones de dólares en quince años. Ciertamente no hubiéramos sacado mucho provecho de esto. En los quince años transcurridos desde este primer éxito, las tecnologías inventadas, perfeccionadas, implementadas y sustituidas por otras mejores, permitieron un progreso asombroso: hoy es posible descodificar un genoma humano en su totalidad por dos mil dólares y en el plazo de un par de semanas. Pero el avance tampoco acaba aquí, y es posible prever la disponibilidad en los próximos diez años de tecnologías que permitirán la descodificación de un genoma por menos de diez centavos y en solo un segundo. Cabe pensar en las transformaciones que comportará este tipo de cambio en el ámbito de la sanidad, los seguros, la privacidad y muchos otros más.</p>
        <p>El mensaje es que no hay nada mágico en un determinado límite al que acabamos llamando unidad o 100 %, y que fue el poder del invento y de la implementación el que hizo que las tecnologías alcanzaran este umbral no se detuvieran sino que continuaran, ofreciendo mejoras en el resultado deseado a precios más bajos y a una mayor velocidad.</p>
        <p>Inteligencia artificial</p>
        <p>La Naturaleza de la Inteligencia</p>
        <p>El núcleo de este libro podría ser un análisis filosófico de la inteligencia propiamente dicha, y si es posible su descripción en términos científicos, o bien si representa una esencia irreductible e irreproducible.</p>
        <p>De hecho, esta ha sido la labor, bajo diversas formas, de los filósofos durante miles de años. Con la inclusión de la naturaleza de la verdad, la belleza, el bien y el mal, la moralidad, la ética y la estética, la filosofía tenía todo lo necesario para establecer unas consideraciones prácticas. Por el contrario, la división del conocimiento en dos campos, el de la comprensión abstracta y el de las consecuencias prácticas, habiendo sido este último desaprobado y considerado inferior por los seguidores del primero, ha sido un puntal de la filosofía occidental desde Aristóteles.</p>
        <p>Este libro contempla varias hipótesis, y el hecho de que la inteligencia pueda ser comprendida, analizada y reproducida es una de las fundamentales. En los últimos libros de los filósofos que no menosprecian ser entendidos por muchos, se incluyen maravillosos argumentos del porqué debería ser así.</p>
        <p>En general, a los efectos de este libro, definiremos la inteligencia como la capacidad de la materia para organizarse de una manera que le permita buscar soluciones a los objetivos mediante la esquematización de una vía de acción pertinente, y para organizar tanto los recursos abstractos como los concretos para alcanzarlos.</p>
        <p>El cerebro humano es una masa de materia que está dotado de un determinado coeficiente de inteligencia. Y utilizo a sabiendas esta expresión en lugar de “mente”. Evitaremos el escollo del dualismo que ha atascado la filosofía intentando entender una mente que se limita a ocupar el cerebro, y que arrastró a Descartes a argumentar sobre los homúnculos y la infructuosa búsqueda del tejido conectivo a través del cual la mente se conecta al cerebro. De acuerdo con este supuesto, el cerebro expresa la mente y la mente es lo que hace el cerebro.</p>
        <p>Durante el siglo XVIII, un fascinante artilugio recorrió las cortes de los reinos de Europa. Diseñado por el húngaro Wolfgang von Kempelen, consistía en una caja grande encima de la cual se sentaba un muñeco de madera; lo que hoy llamaríamos un robot. Jugaba al ajedrez y ganaba a cualquiera que intentara jugar una partida con él, de manera infalible y con precisión mecánica. La afirmación de Von Kempelen de que había construido un autómata capaz de actuar con inteligencia quedó desenmarañada cuando se demostró que la caja albergaba a un enano. Gracias a su dominio del ajedrez, el enano pretendía ser la inteligencia del robot. En realidad era el homúnculo de una configuración artificial de inteligencia mecánica. Constituía una inteligencia artificial, artificial.</p>
        <p>Pruebas de Turing para los humanos</p>
        <p>Cuando en los años 40 nacieron los primeros ordenadores eléctricos, Alan Turing formuló una nueva prueba para la inteligencia de las máquinas. La prueba de Turing, tal como es llamada hoy, afirma que hay razones de sobra para creer que una máquina es inteligente si es considerada inteligente a través de sus acciones por un equipo de jueces humanos. Durante la prueba, la configuración de la prueba oculta la máquina a los humanos, y mezcla la máquina y sus resultados con otros humanos que durante la prueba pretenden ser máquinas y que quieren que los jueces decidan si son máquinas o humanos. Turing lo llamó “el juego de imitación”, probablemente convencido de que era menos trascendente que la fascinación que ahora despierta. Con demasiada frecuencia y de una manera algo grandilocuente, alguna fuente periodística anuncia la superación de la prueba de Turing. Generalmente, cuando se analizan las transcripciones del diálogo escrito entre la máquina ganadora y su juez humano, parece que la máquina, o mejor dicho los programadores que se esconden detrás, hacen bromas, cambian de tema y otras banalidades.</p>
        <p>Ajedrecistas expertos</p>
        <p>En 1996, el entonces campeón mundial de ajedrez, Gerry Kasparov, fue derrotado por una máquina llamada Deep Blue, especialmente diseñada y construida por IBM con este propósito. Decididamente, la máquina no podía intentar superar la prueba de Turing en su juego de imitación basado en el diálogo escrito que abarca temas generales. No obstante, gracias a su hardware y software especializados en el análisis y el descarte de millones de movimientos en el árbol de posibilidades del juego del ajedrez, hasta que decidía cuál era el mejor movimiento en una determinada configuración, fingía sin embargo ser lo suficientemente inteligente en el juego para ganar e incluso enfurecer a Kasparov.</p>
        <p>Si el juego de imitación de la prueba de Turing consiste en convencer a un grupo de humanos de que una máquina puede mantener un diálogo similar al de los humanos gracias al uso de las reglas de sintaxis y semántica, la prueba de Kasparov, el juego del ajedrez consiste en convencer a un jugador de ajedrez que ha perdido contra una máquina que juega con arreglo a las reglas del ajedrez. Y lo que es más significativo, en este caso no existe una diferencia propiamente dicha entre la máquina que pretende saber jugar al ajedrez y alguien que sabe jugar al ajedrez.</p>
        <p>Durante las diversas fases de la competición entre Kasparov y Deep Blue, los ingenieros de IBM retocaron los algoritmos de la máquina, lo que conllevó las enérgicas protestas del campeón ruso.  ¿No era capaz la máquina de aprender durante la partida? Los retoques, que redundaron en un aumento de su inteligencia y posiblemente contribuyeron a su victoria, fueron admitidos por los jueces del torneo.</p>
        <p>Sistemas expertos e inteligencia artificial restringida</p>
        <p>Cuando se aplican a un hardware especializado como la Deep Blue de IBM, o en arquitecturas informáticas más generalizadas, incluso en ordenadores personales, los sistemas que simulan el poder de decisión de un humano en un determinado campo especializado reciben el nombre de sistemas expertos. En la década de los 80, el campo de la inteligencia artificial (IA) estaba dominado por el desarrollo de los sistemas expertos. En ámbitos tan diversos como el diagnóstico médico o la planificación financiera, se codificó en reglas el conocimiento de expertos en una determinada área que eran puestas en marcha por motores de inferencias capaces de aplicarlas con arreglo a los datos proporcionados con el fin de generar una recomendación para una línea de acción: qué posible enfermedad sugerían los síntomas, o qué préstamo era el más apropiado para una determinada situación financiera.</p>
        <p>Estos sistemas expertos mostraron una relativa eficacia, y todavía se utilizan en varios campos, pero no representan un intento de crear un modelo general de inteligencia y no pueden considerarse el paso intermedio del camino hacia una inteligencia artificial general experta en todos los campos.</p>
        <p>Esperanzas y decepciones</p>
        <p>Muchos de los profesionales originales de la IA siguen creyendo en la posibilidad de construir rápidamente ordenadores que muestren una mayor capacidad de pensamiento, creatividad y resolución de problemas. Se aprovecharon de los estimulantes ambientes del mundo académico, desde el MIT (Instituto Tecnológico de Massachusetts) a la Universidad Stanford y otros lugares, para la creación en los años 60 de laboratorios que estudiaran las posibilidades. Y muchos de ellos abandonaron el mundo académico para obtener fondos de la industria o capital de riesgo con la esperanza de instaurar una innovación ampliable y sostenible mediante la aplicación de lo aprendido en los laboratorios.</p>
        <p>La mayoría de las reivindicaciones expresadas, aun teniendo en cuenta el constante desarrollo del hardware disponible que servía de base para los sistemas informáticos de IA, no fueron atendidas.  O al menos no alcanzaron la trascendencia que necesitaban los financiadores para justificar sus continuas inversiones. En la década de los 80 llegó el llamado “invierno de la IA”, y parecía que este escenario no iba a cambiar el mundo con la profundidad que inicialmente se creía.</p>
        <p>Esto es un efecto común de la incomprensión de los exponenciales. El entusiasmo de las personas externas generado por su escaso entendimiento de los principios subyacentes, junto con la premura de los expertos por ofrecer unos resultados consistentes, se concentra en unos cuantos puntos de datos con una interpolación lineal. ¡Pero el crecimiento lineal al principio de un exponencial es en realidad superior! Por consiguiente, aquellos que infravaloran el posterior poder del exponencial están también expuestos a cometer el error de sobreestimarlo al principio.</p>
        <p>El papel del aprendizaje</p>
        <p>Cuando nacieron los ordenadores, su arquitectura inicial no coincidía con la de lo que hoy conocemos como un ordenador. Se parecía más a una herramienta especializada que solo podía utilizarse para un determinado propósito que al instrumento universalmente adaptable que utilizamos diariamente hoy. El hardware era diseñado para ser optimizado y literalmente adaptado a esta tarea individual, y no era factible reconfigurarlo para hacer otras cosas.</p>
        <p>Hasta la aparición al cabo de un tiempo del ordenador con memoria almacenada, implementado con la arquitectura de Von Neumann, que no distinguía entre los números que representan datos y los números que representan instrucciones, no fue posible hablar de un ordenador universal. Incluso entonces se requirió un nuevo desarrollo para completar el concepto de programación del ordenador y de representación de los programas en unos formalismos de mayor nivel, lenguajes abstractos que podían entonces traducirse y recopilarse en el lenguaje de la máquina, y que eran los pasos directamente ejecutados por el ordenador.</p>
        <p>La escritura de estos programas estableció una nueva forma de arte a mediados del siglo XX, y aunque algunos componentes básicos como las ramificaciones y los bucles ya habían sido previamente conceptualizados antes de que se convirtieran en los elementos prácticos de un programa en marcha, se necesitaban unas herramientas más sofisticadas para conciliar unos programas cada vez más y más grandes y garantizar una capacidad de ejecución sin problemas.</p>
        <p>Cómo podían escribirse los programas, y si los propios programas podían ser considerados como datos, con otras partes del programa reescribiéndolos en caso de necesidad y cambiando el funcionamiento del programa principal en curso de ejecución, fue una de las cosas que Turing consideró y comparó con el papel del aprendizaje en los humanos.</p>
        <p>Si somos capaces de construir un programa que juega al ajedrez, otro que hace diagnósticos médicos u otro especializado en recomendaciones financieras, ¿podríamos construir programas que fueran eficaces en todas estas cuestiones y en otras más? ¿Podríamos construir un programa que al ejecutarse en un ordenador sofisticado y potente pudiera enfrentarse a un problema, a cualquier problema, y fuera capaz de analizarlo, de obtener los recursos necesarios y de resolverlo? ¿Seremos capaces de construir un programa que identifique y resuelva los problemas, en un sentido absolutamente universal, en el que su programación no sea inamovible sino que se adapte variablemente en función de las necesidades representadas por el entorno? Esto es lo que llamamos Inteligencia General Artificial.</p>
        <p>Inteligencia General Artificial</p>

        <div style="margin-top: 50px; padding-top: 30px; border-top: 1px solid var(--border-color); text-align: right;">
            <a href="03-cambio-exponencial.html" style="color: var(--primary-color); font-weight: 600; text-decoration: none;">
                Next: Chapter 3: Cambio exponencial →
            </a>
        </div>
        </div>

        <div class="footer">
            <p>&copy; 2018 David Orban</p>
            <p><a href="https://davidorban.com">davidorban.com</a></p>
        </div>
    </div>
</body>
</html>