<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cambio exponencial - Something New: AIs and Us</title>
    <link rel="stylesheet" href="../../styles.css">
</head>
<body>
    <div class="container">
        <div class="book-header">
            <div>
                <h1>Something New: AIs and Us</h1>
            </div>
            <div class="book-nav">
                <a href="../../" class="lang-switcher">← Home</a>
                <a href="../index.html" class="lang-switcher">← All Chapters</a>
            </div>
        </div>

        <div class="book-content">
            <h2>Cambio exponencial</h2>
        <p>. Al cabo de siete años solo se había alcanzado un 1 % del progreso. Incluso los expertos en la materia creyeron que el proyecto había fracasado y que se necesitaría no tan solo siete años más, sino docenas o cientos de años más para completarlo, lo que supondría unos costes astronómicos muy superiores a los fondos originalmente previstos. Muy pocas personas consideraron que el resultado del 1 % había sido obtenido gracias a la duplicación de la capacidad de descodificación. Gracias al mantenimiento de este ritmo, no la velocidad, sino la aceleración del proceso, al año siguiente se había logrado un avance del 2 %, luego del 4 %, el 8 %, el 16 %, el 32 %, el 64 %... y después de exactamente siete duplicaciones más, se alcanzó puntualmente el objetivo del 100 % a los quince años y con arreglo al presupuesto.
No obstante, la duplicación del poder del proyecto no se detuvo aquí; la velocidad de la descodificación aumentó, lo que redujo su coste. Hoy, en 2015, es posible tener un perfil de ADN completo por unos 2000 dólares, o por 99 dólares si se trata de un perfil parcial, de los llamados Polimorfismos de Nucleótido Único (o SNIP, por sus siglas en inglés), que eran considerados los responsables de las características individuales que nos diferencian de los demás. En el ADN humano hay quinientos mil SNIPs y la empresa 23andMe se dedica a su análisis y procesamiento. 
Los resultados son sorprendentes. Mediante el análisis de mi ADN, 23andMe puede decirme el color de mis ojos y de mi pelo, así como docenas de otras características de mi fenotipo; es decir, la manifestación física de la acción de mi ADN. Pueden hacer una valoración estadística en términos de mayor o menor predisposición a unos determinados tipos de enfermedad. Ofrece instrucciones sobre las dosis incluso de fármacos típicos en caso de que deba utilizarlos, además de las recomendaciones de las prescripciones habituales, o por el contrario, me recomienda una dosis más baja debido a mi reacción natural a los componentes del medicamento.
Evidentemente, poder tomar decisiones sobre la base de esta información puede tener consecuencias importantes. Saber que algunos cambios concretos en mi estilo de vida pueden reducir la probabilidad de que sufra una determinada enfermedad, puede cambiar mi vida. Decirle a mi médico que soy especialmente sensible al fármaco que me está recetando para que me ajuste la dosis como corresponde, podría salvar mi vida.
La FDA consideró que la manera en que 23andMe presenta la información, con correlaciones probabilísticas y estadísticas entre el genoma, la conducta y el desarrollo de unas determinadas enfermedades y dolencias explícitas, no era apropiada. Concretamente ha dictaminado que los consumidores no deberían tener acceso directo a esta información, y que solo debería proporcionarse a los médicos, convirtiéndose así en las únicas personas que pueden interpretar los datos y aconsejar a los pacientes basándose en sus propias conclusiones.
En 1517, Martín Lutero clavó sus 95 Tesis en la puerta de la iglesia del Palacio de Wittenberg. Su acto simbólico fue el detonante para el desarrollo del movimiento Protestante en la iglesia cristiana, lo que desembocó en un cisma con el Catolicismo que todavía hoy persiste. Una de las tesis de Lutero era que la Biblia, el texto sagrado del Cristianismo, debía ser traducida del latín al alemán para que la gente pudiera leerla por sí misma, sin la intermediación de un sacerdote. Los intereses existentes, así como una justificación conservadora y dogmática, generaron un conflicto insalvable que no tan solo desembocó en un cisma eclesiástico, sino también en cientos de años de sangrientos conflictos.
Hoy la FDA asume el papel desempeñado por el Vaticano en la época de Lutero. No quiere que la gente tenga acceso al texto sagrado de su ADN, traducido del lenguaje de la bioquímica al lenguaje accesible de la tecnología de la información, y ha dictado que la interpretación del texto solo será posible con la intermediación sacerdotal de los médicos, defendiendo así su posición conservadora y dogmática.
El principio proaccionario
Nuestras acciones determinan el futuro. Las consecuencias de nuestras esperanzas y ambiciones se extienden más allá del presente.
El principio de precaución establece que antes de adoptar una determinada solución, es necesario considerar todo el daño que puede causar. Invocado frecuentemente en las áreas de protección del consumidor, los reguladores se sienten facultados por este principio para cerciorarse de que los nuevos productos y servicios comercializados no tan solo sean útiles y tengan efectos positivos, sino que puedan excluirse los efectos negativos. Especialmente en los campos de la investigación sanitaria y farmacéutica, los principios de precaución han sido la principal fuente de inspiración del desarrollo de productos.
Obviamente, en un mundo ideal, los reguladores solo pretenderían el mejor modo de proceder para el bien común, en lugar de implementar burocracias autopropagandísticas. Y en un mundo ideal, los principales protagonistas de un determinado mercado no utilizarían su poder para detener a los neófitos y reducir los peligros de unos factores competitivos desconocidos, que distorsionan las reglas e influyen indebidamente en el proceso. Es evidente que no vivimos en un mundo ideal.
El principio proaccionario, originalmente propuesto por Max More, tiene en cuenta el coste de oportunidad de la inacción y los costes de la propia regulación, lo que deriva en la obtención de un equilibrio más orientado al futuro. Pensando en las futuras generaciones y en el beneficio que obtendrán de nuestras acciones, la demora en la implementación de una nueva tecnología podría tener graves consecuencias.
La libertad de experimentar, así como la posibilidad de que los individuos adquieran conocimientos fuera de las vías de investigación oficialmente autorizadas, la objetividad, la transparencia y otros componentes del principio proaccionario hacen que sea una herramienta útil para planificar acciones novedosas.
Una familia residente en los Estados Unidos se vio afectada por una enfermedad rara para la que no había una curación comercialmente disponible. La empresa farmacéutica que en realidad acometió la investigación inicial sobre la misma consideró que no merecía la pena someterse a la carrera de obstáculos reguladora para comercializarla. Obviamente, para las personas directamente afectadas por la enfermedad no se trata de considerar los beneficios. Gracias a las comunicaciones modernas y la disponibilidad de la investigación, la familia ha podido conectar con otras familias en su misma situación para comprar los derechos de desarrollar la curación y aplicarla con éxito en sus propios miembros y en los de otras familias igualmente afectadas.
Si hubiéramos tomado como base los planteamientos tradicionales, es posible que esto no hubiera sucedido nunca, tanto desde un punto de vista tecnológico como regulador. Hay innumerables ejemplos de experimentaciones más atrevidas y de libre investigación que están a la espera de la aplicación del principio proaccionario para alcanzar sus objetivos y florecer.
En la consideración de las cuestiones medioambientales, la Unión Europea incorporó el principio de precaución en sus tratados fundamentales. ¿Supondrá esto una mayor probabilidad por parte de la UE de abstenerse de adoptar tecnologías que sí adoptan otras áreas socioeconómicas?  ¿Será esta la base de un cierto nivel de fosilización de la sociedad europea? ¿O quizás es la expresión de esta situación que de realmente le aflige?</p>
        <p>Civilizaciones competentes o incompetentes
¿Es la libertad una propiedad que emerge de la cuestión de la autoorganización? Creemos firmemente que estamos dotados de libre albedrío, y la mayoría de nuestras estructuras sociales se basan en ello.  No hay un fundamento físico en ello.  El determinismo de las leyes físicas, a pesar del quantum de las incertidumbres, no deja lugar a que el concepto oculte y muestre sus efectos. Al igual que nos sentimos constantemente conmovidos por los objetos, animales y fenómenos antropomorfizados, estamos obligados a interpretar las decisiones libremente adoptadas en lugar de ser la consecuencia del estado de la materia y sus interacciones, tanto en nuestro interior como en el exterior.
La conducta individual se añade a la de grupos más grandes, y por último a la de las sociedades. Juzgamos los resultados de las decisiones individuales y las consecuencias en el derecho civil y penal. Podemos juzgar la capacidad de las sociedades para engendrar el bienestar de sus miembros, o por el contrario, para ser corruptas, injustas y generadoras de confusión, violencia y sufrimiento.
La capacidad de un grupo de sociedades para generar bienestar no tan solo depende de la suma de decisiones de sus miembros, también depende del conocimiento del que disponen y a través de quien. La antigua civilización romana alumbró un arte y una filosofía maravillosa, y con razón admiramos hoy sus logros. Sin embargo, estaba organizada, esencialmente, en función del trabajo esclavo, hoy universalmente condenado. ¿Podría haber sido distinto? ¿Es posible imaginar una civilización romana que no empleara la esclavitud? En absoluto, porque el nivel de conocimiento, y sobre todo la disponibilidad de energía que este conocimiento generó, impedían el logro de estos objetivos sin recurrir a la fuerza del músculo humano, o humanos programables, personas a las que se les puede decir lo que deben hacer y lo hacen sin rechistar.
Durante su período de expansión, mientras fue capaz de aprovecharse de inagotables oleadas de esclavos, la civilización romana parecía estar perfectamente adaptada. Se trataba de una mera apariencia porque eso no podía durar. Roma no podía seguir expandiéndose porque básicamente había conquistado todas las tierras disponibles de lo que hoy llamamos Europa, norte de África y Oriente Medio, y esclavizó a todos los individuos que podían ser esclavizados de estas poblaciones. 
En ese momento empezó a decaer y fue incapaz de resistir la llegada de cambios o de adaptarse a ellos. Obviamente, esta es una representación enormemente simplista de una larga y compleja historia. Hay otras muchas fuerzas en juego además de los esclavos o la ausencia de esclavos.
Hoy, con el conocimiento actual, podemos construir sociedades y civilizaciones sin esclavos. Gracias a nuestra confianza en la energía química y muy pronto en la solar, nuestras decisiones vienen determinadas por un uso más eficiente y desplazan a otras posibles alternativas. Nosotros no somos moralmente superiores a los romanos como seres humanos individuales, nos basta con aprovecharnos de la información acumulada y de sus aplicaciones. El desenlace de la Guerra de Secesión estadounidense entre el Sur y el Norte vino dictado por la eficiencia económica y una mejor organización de las bases energéticas e industriales por parte del Norte.
Por consiguiente, nuestra civilización actual es la expresión de nuestro conocimiento. Las tecnologías de las que disponemos la conforman, del mismo modo que la civilización romana fue modelada por el conocimiento y la tecnología disponible en la época. Podemos empezar preguntándonos cuáles son los límites de la adaptabilidad de nuestra civilización, y cómo cambiará con la acumulación de información y su aplicación a los nuevos conocimientos y las nuevas tecnologías. Si hubiéramos preguntado a un romano que nos dijera si era posible construir una civilización sin esclavos, la respuesta hubiera sido “¡No!”. ¿Cuáles son los falsos axiomas que mantenemos? ¿Cuáles son las preguntas que podemos hacer y asumir que las respuestas sean universales, con todo el mundo creyendo firmemente que una determinada suposición forma parte necesariamente de nuestras sociedades en cualquier lugar y en cualquier época? Con el conocimiento del que dispondremos en el futuro, esta respuesta enérgica y falsa nos hará parecer tan primitivos e ingenuos como nos parecen hoy los romanos. 
Cuándo se produce el cambio, y cómo se manifiesta de por sí, depende de las tensiones acumuladas entre las sociedades en una determinada época. Lo que es posible en un lugar puede no ser inmediatamente posible en otro. Las diferencias constituyen una consecuencia dada la aplicación del conocimiento y la acumulación de experiencia. En un mundo de comunicación global como el de hoy, la comprensión de estas diferencias conlleva la posibilidad de aplicar los conocimientos con mayor rapidez, la adopción de las mejores prácticas y de lo que funciona bien y la prevención de errores. Cuando las barreras de comunicación o ideológicas obstaculizan el recorrido de este flujo de información, las divergencias entre las sociedades aumentan. Las tensiones se acumulan, y bajo una aparente inmovilidad, la estructura organizativa de la sociedad está sometida a un estrés cada vez mayor. En ese momento, un pequeño cambio en las condiciones marco pueden comportar enormes cambios fundamentales que se extienden por toda la sociedad.  
Esto es lo que sucedió, literalmente, con el Muro de Berlín, que metafórica y físicamente protegía las economías planificadas de la Unión Soviética y de Europa del Este de las de Occidente. Cuando cayó el muro, los efectos de permitir una rápida penetración de las economías de mercado trajeron primero un cambio económico y posteriormente un cambio político que no pudo ser contenido o controlado ni por las mismas personas que lo iniciaron y permitieron, como el entonces Secretario General del Partido Comunista, Mikhail Gorbachev.
Dado que las diferencias de información crean estas áreas de conocimiento limitado, los individuos pertenecientes a esas áreas a menudo no se dan cuenta de que viven en una sociedad mal adaptada. Pueden quedarse muy sorprendidos cuando la debilidad y la fragilidad de la civilización quedan evidenciadas por los cambios bruscos. Incluso los expertos políticos y los historiadores explican mejor los rápidos cambios de las civilizaciones una vez han sucedido que cuando los prevén. Esto dificulta la preparación para los cambios y la mitigación del sufrimiento que genera el período de incertidumbre. 
La insostenibilidad es insostenible
En los últimos 200 años ha predominado el actual paradigma económico capitalista basado en el crecimiento constante. Incluso antes, la explotación de los recursos y su adjudicación a las sociedades feudales permitieron ignorar lo que hoy llamamos las externalidades de las actividades económicas. Esto fue posible porque a las naciones o civilizaciones no les importaba destruir a los competidores. O si partimos del supuesto de que con el agotamiento de los ecosistemas y la exterminación de las especies dominantes de un determinado continente, siempre quedaría algo por descubrir y para empezar el proceso de nuevo.
Hoy es evidente que esta conducta ya no es posible. Los países avanzados no deberían librar guerras entre sí. No deberían destruir o someter a otras poblaciones. No deberían poner en peligro los ecosistemas y sus especies con sus actividades económicas. En pocas palabras, no quedan más continentes por saquear.
Esto significa implícitamente que no hemos mejorado como personas. No cambiamos nuestras costumbres porque entendíamos que debíamos adoptar una conducta moralmente superior. Nuestra mentalidad sigue siendo básicamente la misma. La razón por la que ahora estamos considerando alternativas es porque ya no es posible mantener las viejas usanzas.
Las externalidades de nuestras actividades económicas son todas aquellas consecuencias que no se ven reflejadas en las consideraciones relacionadas con las ganancias y las pérdidas. Le corresponde a la sociedad analizar diversos sectores y decidir si puede permitirlo. Como alternativa, puede implementar normativas que afloren los costes ocultos y dejar que la sociedad en su conjunto los absorba explícitamente o, por el contrario, obligar a las empresas que operan en la cadena de producción a afrontarlos. 
Las prácticas económicas insostenibles poseen grandes externalidades, lo que no es permisible en un mundo cerrado y globalmente conectado. Para solventar el agotamiento de los sistemas de soporte ecológicos, y el posible uso o reciclaje de los residuos de recursos de una manera más eficaz, una sociedad compleja debería dirigirse hacia unas prácticas sostenibles con el fin de poder generar soluciones dinámicas y consolidadas a la vez.
Una civilización no puede considerarse perfectamente adaptada si no reconoce esta necesidad y si no actúa al respecto con las herramientas de los incentivos y las normativas apropiadas.
Los métodos de conocimiento
Para sobrevivir debemos observar nuestro entorno, tratar de entenderlo, adquirir los recursos que necesitamos y planificar las acciones necesarias para el logro de nuestros objetivos. Si sabemos cuáles son las reglas del mundo, sistematizamos el conocimiento y entendemos cómo podemos adquirir un mejor conocimiento, lo que nos puede ser de gran utilidad para ser mejores en la supervivencia.
El fatal error de los alquimistas
En un mundo dominado por la competencia y en el que la percepción universal es que los recursos son escasos, es natural optar por una estrategia de secretismo. La recopilación en secreto de conocimientos supone una ventaja para quienes la explotan contra otros. Un sistema de conocimientos cerrado y fuertemente custodiado es una barrera que los demás deberán superar si quieren participar al mismo nivel.
Al mismo tiempo, un sistema cerrado y secreto es también vulnerable al aislamiento y al sufrimiento de sus propios errores al aislarse. Habida cuenta de su incapacidad para compartir sus aprendizajes, una comunidad que confía en el secretismo está abocada a repetir los errores por no haber compartido sus lecciones.
Los alquimistas medievales que les obsesionaba el propósito de transformar el plomo en oro eran lo suficientemente desdichados para creer que el uso del mercurio les ayudaría en esta cruzada. Desgraciadamente, el mercurio es venenoso. Los alquimistas que conocían este hecho lo hicieron a escondidas y sufrieron las consecuencias de este conocimiento. La organización de las sociedades secretas impidió la posibilidad de aprender de los errores ajenos, y los alquimistas estaban destinados a repetirlos.
En el mundo actual todavía hay muchas actividades que son realizadas también en secreto. Se sospecha que el intercambio del conocimiento debilita la posición de los que compiten por los recursos.
Ciencia abierta
La revolución científica que inició Galileo no tan solo supuso una comprensión más evidente de la necesidad de que la teoría y los experimentos estuvieran relacionados, también allanaron el camino para un cambio profundo en la recopilación, comprobación y divulgación del conocimiento y la información. Aunque no sea una parte necesaria del método científico en sí, la colaboración abierta permitió que los grupos y los individuos que supieron sacar provecho de ella pudieran decidir con mayor rapidez la fiabilidad o incerteza de una determinada serie de resultados.
La ciencia abierta es sustancialmente superior a los planteamientos cerrados de recopilación de conocimientos. La colaboración entre las personas que comparten los objetivos y las pasiones en un determinado ámbito es impulsada por un lenguaje y unas herramientas comunes. La publicación de los resultados obtenidos por parte de un grupo permite que otro grupo pueda realizar experimentos contrapuestos al nuevo conocimiento para confirmarlo o rebatirlo. La colaboración interdisciplinaria es engrandecida por la naturalidad con la que los profesionales ajenos al grupo pueden abordarla gracias a los puentes de interpretación tendidos que prevalecen sobre las especializaciones.
Hoy disfrutamos de la posibilidad de una prolífica área de estudio, los metaestudios, gracias a la creciente digitalización de la ciencia y de la publicación científica. La comparación y el análisis de un gran número de publicaciones en un determinado ámbito facilitan la obtención de resultados no contenidos individualmente en ninguna de ellas.  Las herramientas estadísticas revelan unas tendencias relevantes e incrementan la posibilidad de detectar, y eventualmente corregir, los errores metodológicos existentes en el trabajo anteriormente publicado.
El mundo de la ciencia valora los hechos, las teorías, las verificaciones y los experimentos, así como la publicación de resultados para el intercambio de conocimientos. Teniendo en cuenta que es uno de los principales intereses del científico, aunque no el único, en el que basa sus progresos y su capacidad para recibir ayudas económicas, las publicaciones científicas, a través del sistema de revisión paritaria, han sido la esencia del desarrollo de la ciencia. 
El valor del artículo científico se mide por el número de otras publicaciones que lo citan y por la importancia de la revista en la que se publica. Esto ha sido convertido directamente en valor económico por las editoriales de las revistas científicas, que han creado empresas considerables gracias a las suscripciones de las universidades y centros de investigación a sus publicaciones en las que aparecen los artículos de los científicos. El precio de estas suscripciones ha experimentado tal aumento que las universidades de países de renta baja y media, o incluso algunas de países de renta alta, no pueden hacer frente a la tarifa de suscripción. Cuestión más importante es que la investigación financiada con fondos públicos y sus resultados sufragan los ingresos y los modelos empresariales de las empresas privadas que recopilan los artículos y que acaban cobrando dos veces.
Han empezado a surgir publicaciones de libre acceso que han adquirido popularidad y prestigio, que ofrecen modelos alternativos a la publicación científica y en las que el lector de los artículos no paga por el acceso ni por la suscripción de la revista. Con el sistema de revisión paritaria que se sigue aplicando en el control de calidad del artículo, el autor del artículo y el centro al que pertenece, pagan por la publicación una tarifa asequible y razonable que se añade como una simple partida presupuestaria al presupuesto de un experimento o a la solicitud de financiación.
Con la crueldad que le caracteriza, la ciencia reevalúa también la eficiencia de su propia revisión paritaria, e intenta por un lado medirla y por el otro busca posibles alternativas para garantizar un alto nivel de calidad de las publicaciones científicas. El análisis de la estructura de los experimentos científicos contempla la posibilidad de que los resultados publicados puedan ser doblemente verificados mediante la reproducción y verificación independiente de los datos.
No tan solo el artículo científico propiamente dicho, sino con las notas de laboratorio y los flujos de datos primarios subyacentes para una posterior evaluación e incorporación, se ha convertido en una norma en varias áreas. Gracias a la generación de unas series de datos cada vez más grandes en el ámbito informático, en las ciencias biológicas y en muchos otros campos, el análisis de los datos se ha convertido en una nueva e incipiente oportunidad por sí misma.
Los macrodatos y la ciencia de datos ofrecen la posibilidad de incorporar y sublimar el valor a partir de grandes acopios de información estructurada o desestructurada. Su aplicación es mejor entendida por la genética, el Internet de las Cosas, la sociología y otros campos. Por un lado, las ciudades y los gobiernos pueden poner a disposición de otros los flujos de datos que generan, sin necesidad de permisos y libremente, en un entorno que promueve la innovación y la creatividad, y por otro lado, pueden aprovechar los resultados para aumentar la transparencia, la responsabilidad y la eficiencia de sus operaciones. 
La evolución de la ciencia
La disponibilidad de una creciente cantidad de datos y la interconexión del mundo con sus expertos, facilitan la creatividad en la forma de encajar las piezas del puzle científico.
En el tradicional proceso de los estudios universitarios, de postgrado, doctorado y postdoctorado, los crecientes niveles de especialización se caracterizan por el trabajo, lo que comporta, prácticamente por unanimidad, una menor accesibilidad. Una alternativa en profundidad a este proceso es el menos desarrollado y organizativamente complejo enfoque en amplitud, en el que la colaboración interdisciplinaria y la fertilización cruzada de varios campos, está siendo objeto de atención para producir unos resultados innovadores.
En campos como la cosmología, donde solo disponemos de un universo y la organización de los experimentos escapa a nuestro control, puede observarse un fenómeno de muy alta energía profundamente conectado con las teorías de la física de partículas, que posiblemente hubieran necesitado máquinas con un tamaño de energía y un precio excesivos para poder ser construidas en la Tierra. 
Los estudios comparativos que promueven el potencial de las publicaciones de libre acceso, el de la elaboración de flujos de macrodatos y el del texto desestructurado de los propios artículos, resaltan confirmaciones o anomalías estadísticas en estudios a lo largo de los años, en varios centros de investigación y en el trabajo de muchos científicos. Este metaconocimiento puede fomentar un valioso entendimiento de la reproducibilidad, la eficiencia y las prometedoras áreas de investigación con el fin de dedicar mayor atención y recursos.</p>
        <p>Cada vez se presta más atención epistemológica a la estructura de las teorías, lo que confirma el legítimo cuestionamiento del excesivo poder generativo de algunos grupos de teorías. La teoría de cuerdas, una familia de las teorías de la física de partículas es capaz de sacar de su sombrero epistemológico una teoría que responde a un resultado experimental cualquiera, dado que abarca de 10 a 500 teorías aproximadamente (millardos y millardos… de teorías) con una cuestionable aplicación de la secuencia de la teoría, la predicción y la verificación.
El elemento humano de la estructura de las teorías científicas y su evolución se está entendiendo más a fondo. Por naturaleza, la capacidad de incorporar nuevas y arriesgadas áreas de investigación es más probable al comienzo de la carrera de un científico. La cada vez mayor longevidad de algunos prestigiosos líderes del ámbito académico debe ir a la par con un análisis de cómo conservan su agilidad y su aceptación del riesgo cuando los recursos son asignados a enfoques diversos, y con la cesión de la palabra a nuevas incorporaciones y nuevas ideas en sus respectivos ámbitos.
Cambio exponencial
Sistemas dinámicos
Todos los fenómenos son dinámicos, y aunque solemos intentar analizarlos y oficializarlos a través de un proceso opuesto, la mayor representación de la realidad es la abstracción estática. Evidentemente, tal como lo han reflejado las contiendas en la evolución de los métodos científicos, la realidad no siempre es fácilmente descifrable. Ni tampoco nos ha sido convenientemente presentada en paquetes intuitivos adaptados a nuestro sentido común. Muchas de nuestras intuiciones sobre las reglas que rigen los fenómenos naturales han resultado ser erróneas. Esto es lo que puede suceder con una serie de experimentos que, una vez que las leyes físicas que subyacen detrás del fenómeno son perfectamente comprendidas, cualquiera puede llevarlos a cabo. Llegados a ese punto, a través de unas explicaciones accesibles que pueden ser ilustradas con inmediatez, no tenemos excusa para seguir ignorando la naturaleza de la realidad.
Un sencillo ejemplo es la primera ley de Newton, que dice que “todo cuerpo permanece en su estado de movimiento a menos que una fuerza externa actúe sobre él”. Nuestra experiencia diaria es que, para ser exactos, un coche en marcha se detendrá si el conductor deja de apretar el acelerador. Pero ahora entendemos con toda claridad el papel del desgaste, y que la deceleración es fruto del motor, del terreno y del aire en contra. Si elimináramos todas las causas del desgaste, el coche seguiría moviéndose eternamente.  
Las consecuencias del cambio dinámico están a nuestro alrededor, en las variaciones de agua, ríos, océanos y lluvias. En el crecimiento de la vegetación, árboles y bosques, o en el avance de los desiertos y los cambios de estaciones. Pero aunque contamos con una dilatada experiencia en el cambio dinámico, nuestra intuición puede ser más engañosa que nunca en lo que respecta a la potencia bruta que se esconde tras su naturaleza matemática abstracta, desenfrenada y descontrolada por los imperativos de un entorno físico y natural.
El cambio exponencial es un entorno como tal dinámico. Podemos prepararnos, pero su fuerza contundente nos seguirá sorprendiendo e incluso a menudo confundiendo a los expertos, y sin duda cogerá desprevenidos a los legos por su impactante remodelación del paisaje de nuestra realidad. 
Secuencias trascendentes
Los ejemplos más sencillos del cambio exponencial, por ejemplo la duplicación de una cantidad en un determinado período de tiempo si el punto de partida es la unidad de 1, pueden parecer bastante inofensivos o incluso decepcionantes al principio.
1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096,… es una secuencia familiar incluso para quien no haya mostrado un especial interés por los números. El traqueteo de la secuencia en la cabeza era un ejercicio inofensivo propio de la infancia.
Existe una secuencia correspondiente, antes de la unidad, que podemos considerar potencialmente sencilla incluso en mayor grado: 0,01, 0,02, 0,04, 0,08, 0,16, 0,32, 0,64, y por último, 1,28. La curiosa sorpresa de esta secuencia, nada mágica por cierto, es lo que le precedía: 0,00015625, 0,0003125, 0,00625, 0,00125, 0,0025, 0,005, 0,01.
¿De dónde proviene el interés por estas tres secuencias y por qué son representativas de la naturaleza del cambio exponencial?</p>
        <p>Imagine que usted está mirando el mundo que le rodea, y en su intento por descifrarlo y predecir la consecuencia de un determinado fenómeno, usted recoge datos sobre el mismo. Esta recopilación no será tan nítida y clara como las secuencias anteriores. El griterío está asegurado. Errores de medición, errores cometidos durante el proceso o la planificación de las mediciones, otros fenómenos que se inmiscuirán y confundirán sus intentos por alcanzar una clara comprensión, etc.
El griterío de un entorno natural donde, incluso antes de entrenar sus oídos para ello, usted percibe un modelo que posiblemente sea nuevo, algo que nadie hasta ahora intentó escuchar.
Señales ruidosas
¿Hay señales en el ruido? Es muy probable que mientras usted intenta responder la pregunta, aparezcan otras opiniones a su alrededor. Y por definición discreparán de la suya, no coincidirán o incluso serán opuestas. Si usted está inmerso en un entorno de investigación y compite por conseguir una ayuda económica, o trabaja en la industria y el producto que intenta diseñar o el servicio que promociona entre usuarios legítimamente trastornados por una descomunal oferta de opciones alternativas, en ambos casos se sentirá confundido por la resistencia mostrada a su original teoría sustentada por las señales.  Debe ser categórico en sus opiniones, incluso debe tener fe en lo que quiere demostrar, una convicción irrazonable de que tiene razón aunque todo el mundo le diga que está equivocado. O aun cuando lo que esté buscando no exista o sea imposible.
Este es el ámbito de la tercera secuencia, que conduce hasta el 0,01 (o 1 % de la unidad). La dimensión en la que incluso los expertos estarán en su contra. Hace falta tener buen ojo y buen oído, en sentido concreto o abstracto, para entender que frente a la presencia de las distracciones de un mundo de por sí ruidoso, hay algo que está fermentando. La duplicación serena, sin que nadie se dé cuenta salvo usted, para después de varias duplicaciones alcanzar el umbral del 1 % partiendo del objetivo de la unidad. 
Una vez haya llegado al 1 %, estos expertos que siguen sin creerle deberían ser ignominiosamente despojados de su etiqueta. Porque a partir de entonces debería quedarle claro, no tan solo a usted sino a todo el mundo que preste una atención incluso fugaz, de que se trata de una mera cuestión de tiempo. En solo siete duplicaciones sucesivas, usted habrá llegado a la unidad.</p>
        <p>El crecimiento exponencial del Proyecto Genoma Humano 
Si convertimos esta descripción de las secuencias abstractas de números en un ejemplo real, podemos ver lo que sucedió en el gigantesco Proyecto Genoma Humano realizado en Estados Unidos. Iniciado en 1985, la duración del proyecto se estableció originalmente en 15 años. Al igual que cualquier proyecto científico, no quedaba muy claro cómo se salvarían todos los obstáculos, ni cuál sería el enfoque agraciado. ¡A los siete años de iniciar el proyecto solo se había alcanzado el 1 % del objetivo! En aquel momento fueron muchos los que exigieron en voz alta la suspensión o incluso el abandono del proyecto: ¡miren, estamos a medio camino y solo se ha alcanzado el 1 % del objetivo! Aquellos que fueron más prudentes, o los expertos en dinámica exponencial, entendieron que todo iba bien. El logro de la duplicación del 1 % de los pares de bases descodificados cada año durante los siete años anteriores significaba que en otros siete años de duplicaciones, el proyecto alcanzaría su objetivo del 100 % de descodificación de la totalidad del genoma humano.
Son muchos los fenómenos que están sometidos a un crecimiento exponencial: poblaciones y reacciones nucleares en cadena, por nombrar solo un par de ejemplos. Las poblaciones crecen exponencialmente porque, habida cuenta de que, generación tras generación, las parejas tienen de promedio más de dos hijos, el incremento será acumulativo: aquellos descendientes tendrán también más hijos. Las reacciones nucleares en cadena se producen también cuando los materiales fisibles, el uranio por ejemplo, entre otros productos de fisión produce neutrones, que antes de abandonar el volumen de material generan otra descomposición aparte de átomos de uranio, lo que produce otros neutrones, y así sucesivamente.
Lo más importante para el tema de este libro, el poder de los sistemas informáticos y de la información, también está creciendo exponencialmente, y lo lleva haciendo desde hace más de 50 años. Sin embargo, no hay una ley natural detrás de esta dinámica, ni una necesidad biológica o física. Es un proyecto de ingeniería que acabó recibiendo el nombre de la persona que lo formuló por primera vez, la Ley de Moore.</p>
        <p>La Ley de Moore
Gordon Moore trabajaba en el circuito integrado recién inventado a comienzos de la década de los 60. Estaba metido en un entorno ruidoso, en lo que se refiere a la necesidad de concentrarse en las particularidades de un nuevo fenómeno en presencia de otras muchas personas que trabajaban simultáneamente en ello. Los ordenadores prácticos irrumpieron hace más o menos un par de décadas, y cada vez son más y más potentes, pero a un ritmo que resultó ser bastante lento si lo analizamos linealmente.
Se intentaron varios planteamientos para conseguir almacenar más información para los cálculos y para ejecutarlos con mayor rapidez. Los tubos al vacío, las memorias de núcleos magnéticos y otros componentes a los que los medios de comunicación de la época bautizaron con el nombre de “cerebros electrónicos”, eran engorrosos, mostraban unos altos índices de fallos y necesitaban veintenas de especialistas que se encargaran de ellos para cerciorarse de su funcionamiento. El coste de los ordenadores era de millones de dólares y, solo los programas nacionales de investigación o las empresas más grandes, podían permitírselos.
El invento del transistor para ser utilizado como componente básico para el cálculo, abrió las puertas a una producción, montaje y funcionamiento de los ordenadores mucho más fiables y baratos. Los transistores podían ser empaquetados junto con otros componentes y así crear una valiosa unidad de cálculo llamada circuito integrado. No tan solo eso, sino que dada su naturaleza, existía la posibilidad de predecir el desarrollo de unos componentes de próxima generación más pequeños, más rápidos y más baratos que los anteriores.
Gordon Moore supo advertir cuáles eran las posibilidades actuales de los procesos de producción, así como el incremento de estas posibilidades en pocos años. Basándose solo en un puñado de puntos de datos trazados en un trozo de papel cuadriculado que todavía sobrevive después de 50 años, formuló con valentía la predicción de que el número de transistores que podía alojarse en un determinado circuito integrado se duplicaría cada año. Posteriormente corrigió la predicción a dos años, y finalmente la estableció en 18 meses, que es el valor actualmente aceptado y utilizado.
Teniendo en cuenta que solo disponía de unos pocos puntos de datos, esta predicción era bastante audaz, por no decir imprudente. Sin embargo, desde la perspectiva actual, parece que esta intrépida ambición es lo que verdaderamente se necesitaba. Porque lo que sucedió es que, espoleado por la curiosidad, el deseo de sobresalir y la competencia económica esencial, se organizaron más y más grupos de ingenieros para crear circuitos integrados más potentes. Junto con todos los sistemas de apoyo necesarios, juntos tejieron una industria entera. Al comienzo, este proceso se vio impulsado por la capacidad individual de estos grupos y por su capacidad de sacarlo al mercado. No obstante, posteriormente, la Ley de Moore se convirtió por sí misma en una fuerza impulsora, una especie de profecía autocumplida, así como un indicador con el cual medir los logros de los diversos grupos.
Se predijo repetidamente que la Ley de Moore fracasaría en su deseo de seguir manteniéndose en la siguiente generación, y tarde o temprano está obligada a hacerlo con sus formulaciones más estrictas. En términos más generales, con la extensión de sus predicciones al poder de la informática hay razones para creer que todavía es posible mantenerla durante mucho tiempo. Con solo cambiar la silicona por otros sustratos para los circuitos, crear componentes tridimensionales, sustituir las arquitecturas que ven un impedimento en los fenómenos cuánticos para quienes las explotan al máximo… existen muchos criterios para superar las posibles barricadas que se encontrarán para demostrar la oportunidad de esta ley, de la misma manera que se han superado otras en los últimos cincuenta años.
Cabe señalar que la divulgación del conocimiento es la base de la Ley de Moore. Ningún grupo individual que trabajara en secreto podría esperar ser el único capaz de resolver los problemas que van surgiendo en el camino de la próxima generación de soluciones, o el único que lo intenta. Esto solo será posible con la colaboración de muchos grupos. Basta con que uno de ellos consiga un avance para el descubrimiento de la solución necesaria. Todos los demás lo aprovecharán gracias a unos acuerdos de licencias que incorporan la solución en las plantas de producción de próxima generación para la producción en masa de circuitos integrados, que hoy son producidos en miles de millones al año.
El complejo ecosistema de enclavamiento de la infraestructura industrial necesario para mantener el ritmo de la evolución en la informática no se reduce a la propia producción de circuitos integrados. También deben evolucionar las herramientas de fabricación para la creación de los circuitos, los sistemas de software que permitan diseñarlos y la ayuda financiera necesaria para invertir en plantas, materias primas, perfeccionamiento, y lo que es más importante… capital humano.
Independientemente de cuales sean los límites físicos fundamentales para el crecimiento del cálculo, ponderado por la generalizada Ley de Moore, todavía quedan muy lejos. Los avances que hemos vivido en los últimos 50 años de crecimiento del poder de la informática van a verse infinitamente eclipsados por los avances de los próximos 50 años. De hecho, en los dos próximos años van a verse superados por la propia naturaleza del crecimiento exponencial, Y de nuevo a los dos años siguientes, y así sucesivamente.
El poder de las duplicaciones
No importa, evidentemente, la rapidez con la que se desarrolla una secuencia exponencial. No es necesario que se duplique en un año para que sea exponencial. Son unidades meramente arbitrarias, y lo hará cualquier cambio acumulativo donde la cantidad resultante aumente en la cantidad expresada en el propio resultado. Si tenemos una cantidad de 100 y ésta aumenta en 10, obtendremos 110, 120, 130, etc. Esto es un crecimiento lineal. Pero si tenemos una cantidad que aumenta un 10 %, entonces la secuencia será 110, 121, 133, y así sucesivamente. Esta pequeña diferencia, que no parece ser demasiado significativa al principio, es lo que prima. Esto es el crecimiento exponencial.
Hay muchas maneras de expresar este poder, y muy sorprendentes para quienes están acostumbrados a razonar linealmente.
Miremos por ejemplo la suma de esta secuencia: 1, 2, 4. La suma 7 = 1+2+4 es la cantidad total de la secuencia entera. Y el próximo paso de esta secuencia es 8, cifra superior al total de todos los pasos precedentes. Esto sucede en todos los crecimientos exponenciales. ¡En el próximo período de duplicación del cálculo, en solamente 18 meses, gracias a la Ley de Moore, habrá más transistores y circuitos integrados creados (y ordenadores fabricados con ellos y cálculos realizados gracias a ellos) que en toda la historia de la informática de los últimos cincuenta años o más!
¿Cuándo es demasiado tarde?
Otro ejemplo que ilustra el poder de los exponenciales es un sistema cerrado, por ejemplo una charca en la que vive una población de ranas. Si las algas hacen que el lago sea inhabitable para las ranas, y estas cubren cada vez una mayor superficie del lago, desde solo una fracción a una duplicación del uno por ciento cada semana, ¿cuánto tiempo deben vivir las ranas antes de que las algas cubran la mitad de la charca? Por suerte la respuesta es clara: ¡solo una semana ya que en la siguiente duplicación el lago quedará totalmente cubierto por las algas! Lo más alarmante, quizás, es que con solo el 1 % quedan menos de dos meses para que las ranas huyan a otra charca, o busquen la manera de parar la propagación de las algas.
Nuestra posición privilegiada nos permite ver que lo que sucede en la charca es perjudicial para las ranas. Y esta capacidad para recopilar datos, analizar y prever supone una enorme responsabilidad para comprender si la charca está en buenas condiciones o no. La adopción de medidas para el control de la charca, el recuento de las algas no puede ni será realizado por otros, pero nosotros sí podemos hacerlo.
Los diversos ejemplos de cambio exponencial que hallamos en la naturaleza se autoalimentan, pero raramente se agrupan en cadenas interactivas que se alimenten recíprocamente. Por otra parte, nuestra civilización tecnológica está llena de cadenas que se retroalimentan y que mantienen la aceleración del cambio exponencial.</p>
        <p>El proyecto de Ray Kurzweil
El inventor, autor y cofundador de la Universidad de la Singularidad, Ray Kurzweil, lleva décadas recogiendo datos sobre el fenómeno exponencial. De hecho, no basta con reconocer lo que sucede. La explosiva naturaleza de los exponenciales es tal que la elección del momento oportuno es fundamental si uno quiere deslizarse sobre la ola en lugar de quedarse viéndola de lejos. 
Súbase demasiado pronto a la ola, y quienes dicen lo contrario lo tendrán fácil para abatir su entusiasmo o el de sus proveedores de fondos, porque el repunte de la secuencia que sigue el rastro de la hipotética unidad de nuestras secuencias de ejemplo, no se producirá. Súbase demasiado tarde, y ya habrá desplegado todo su poder cuando quiera subirse a ella, lo que hará que el esfuerzo sea costoso, difícil o incluso imposible, mientras los demás habrán coronado ya la cresta.
Desde los escáneres planos al reconocimiento óptico de caracteres, desde la síntesis musical a la síntesis de voz o los sistemas portátiles para invidentes, todos los inventos de Kurzweil son fruto de un profundo conocimiento de la elección del momento oportuno. Cuando acelerar la investigación y desarrollo para que en el momento en que los sistemas hardware de apoyo alcancen el precio justo y el nivel de integración adecuado, todos los demás componentes del software, la interfaz de usuario, los sistemas de desarrollo y la totalidad del ecosistema de apoyo estén también preparados.
En el Instituto Santa Fe basado en la investigación de Bela Nagy, hay una base de datos completa de fenómenos exponenciales a la que puede accederse para estudiarla y expandirla aún más. 
Kurzweil reconoció también que con los sistemas recíprocamente interconectados y comunicados de conocimiento humano que no crecen aislados sino que se refuerzan mutuamente, hay exponenciales que se alimentan de exponenciales. A este efecto resultante lo bautizó con el nombre de Ley de Rendimientos Acelerados, que contradice la sabiduría adquirida de la economía clásica en la que se supone que para alcanzar un determinado incremento del rendimiento económico, es necesario que haya una cantidad progresivamente mayor de entrada de capital disponible, la llamada Ley de Rendimientos Decrecientes. 
Al igual que la Ley de Moore, la Ley de Rendimientos Acelerados formulada por Kurzweil es una profecía autocumplida, sostenida por las comunicaciones abiertas y los grupos competidores que aspiran a alcanzar el éxito y la excelencia en sus iniciativas de investigación y de producción industrial. Decididamente pueden infringirse ambas leyes. Si dejamos de creer en la ley de gravitación universal y nos tiramos del quinto piso de un edificio, podríamos hacerlo miles de veces y nunca dejaríamos de caer como una piedra, siendo lo más probable que nos matáramos.  Pero si no hubiéramos intentado hacer unos circuitos mejores, o si hubiéramos decidido que no valía la pena esforzarnos para hacer unos paneles solares mejores, unas mejores baterías, etcétera, siempre y cuando se hubiera detenido también todo el mundo, estos circuitos, paneles y baterías no existirían.  
A fecha de este escrito, Kurzweil es Director de Ingeniería de Google, y según sus propias palabras, el primer trabajo que ha tenido. Gracias al uso de los recursos proporcionados por la empresa, está aplicando sus conocimientos en la creación de una interacción del lenguaje natural con los ordenadores, y en la próxima oleada de interacción del usuario, que hará que los ordenadores sean todavía más fáciles de usar y más capaces de satisfacer nuestras necesidades.
Conexión de las curvas S
Una crítica frecuente del análisis y las predicciones de Kurzweil se basa en un malentendido de lo que constituye el exponencial del que habla. Los críticos destacan el hecho de que lo que parece ser un exponencial es en realidad la primera mitad de una curva S o curva logística. Al comienzo parece un exponencial, dada la explotación de las ventajas de una determinada tecnología. No obstante, al cabo de un rato se desestabiliza porque cada vez es más difícil extraer más ventajas de la misma tecnología. Acaba agotándose y falsea la creencia en el poder de la tecnología de quienes predican unos exponenciales interminables. 
Sin duda es verdad: a lo largo de su ciclo, llega un momento en que todas las tecnologías individuales no dan más de sí. A medida que van acercándose a este límite, es inútil querer insistir en exprimirlas más, tanto desde el punto de vista de la ingeniería como de la economía y la rentabilidad. Y esta es la razón por la que nuevos grupos con nuevas ideas intentarán alcanzar el resultado deseado a través de unos enfoques distintos. Los inteligentes captarán el momento en que se agotará la actual generación de tecnologías y trabajarán en paralelo con los grupos punteros del momento para encontrar una nueva tecnología que alcance el objetivo a escala, y mejor que antes. El ciclo de algunas duplicaciones exponenciales se repetirá y se necesitará una tercera generación de soluciones, y así sucesivamente.  
El efecto acumulativo de estas diferentes curvas S, que allanarán las terminaciones de cada una y que se entrelazarán más o menos a la perfección en una cadena de ingenio, innovación y desarrollo industrial, traza el exponencial que apunta Kurzweil en sus análisis.
Si hablamos de informática, por ejemplo, ha habido un gran número de generaciones de tecnologías informáticas, todas ellas punteras en su momento, que han sido forzadas al máximo y se han visto desbancadas por la siguiente, por otra mejor, más barata y más rápida en la generación del resultado deseado. Década tras década, los relés mecánicos, los tubos al vacío, los transistores y los circuitos integrados han permitido la construcción de los ordenadores más rápidos y más potentes del mundo. Las empresas que los utilizaron fueron líderes en su época y forzaron al máximo las tecnologías, y fueron sustituidas por otros nuevos modelos basados en las nuevas tecnologías en solo unas cuantas duplicaciones del rendimiento.
Otro ejemplo de que en estos años estamos asistiendo a un cambio radical, lo encontramos en el almacenamiento permanente de la memoria. Las cada vez mayores cantidades de datos que deben ser constantemente registradas por nuestros ordenadores para que, cada vez que se interrumpa el suministro eléctrico y el ordenador se despierte más tarde, podamos recuperar los datos sin necesidad de empezar de nuevo desde cero.  Desde las tarjetas perforadas a la memoria de núcleos magnéticos, la cinta magnética y los discos duros giratorios, ahora que estamos a punto de trasladar el almacenamiento por las necesidades de la próxima generación de un soporte de estado sólido (almacenamiento flash), que va a ser capaz de memorizar órdenes de mucha mayor importancia, y al que se podrá acceder de una manera mucho más rápida, fiable y asequible que con cualquier generación anterior de dispositivos. 
Exponenciales por doquier
Son muchas las tecnologías que pueden verse a través de la lente de dicha interpretación exponencial de este cambio acelerado. Los períodos de duplicación pueden ser distintos, obviamente, de los 18 meses establecidos por la Ley de Moore en los que solíamos confiar.
En la energía solar hablamos de la ley de Swanson, que representa el descenso del precio por vatio de un panel fotovoltaico. A partir de 1974, gracias a la creación del primero de estos dispositivos, el coste de más de 70$ por vatio se redujo a 0,30$ por vatio, y el precio continúa bajando. Este descenso es fruto de las economías de escala, de una mejor comprensión de los procesos de fabricación y del nacimiento de un ecosistema de financiación, implementación y mantenimiento de los módulos, así como de los nuevos enfoques básicos de los materiales y de los métodos de construcción, que aumentan considerablemente la eficiencia de un determinado módulo que transforma la luz solar en electricidad.
Hay una duplicación de la capacidad de almacenamiento de nuestras baterías. Esta duplicación es un período de diez años más sosegado (y exasperante si cree que dedica demasiado tiempo a la carga de sus dispositivos ávidos de potencia). Dependiendo de la metalurgia, la química y los procesos de fabricación, cabe imaginar que en la ilustración de la Ley de Rendimientos Acelerados de Kurzweil, la industria encontraría una manera de acelerar la duplicación mediante la adopción de un enfoque radicalmente nuevo y así lograr la práctica aplicación de lo que antes era imposible.
Un 100 % nada mágico
Los objetivos establecidos por los propios programas de investigación tales como el Proyecto Genoma Humano, son a menudo ligeramente arbitrarios. Representan un indicador útil pero no el objetivo del desarrollo de los procesos o de su sofisticación, ni por supuesto el deseo de conocimiento y de la posibilidad de adquirirlo de una manera más rápida y económica. Tras la descodificación del genoma humano de un único individuo se esconde la tarea de hacer lo mismo en otros siete mil millones de personas. Detrás del genoma humano está el genoma de otros animales, o de las bacterias de los océanos, o de las bacterias que simbióticamente viven continuamente en todos nosotros y que constituyen nuestro microbioma.
La capacidad de descodificar el genoma humano no se detuvo a la proporción de uno por tres mil millones de dólares en quince años. Ciertamente no hubiéramos sacado mucho provecho de esto. En los quince años transcurridos desde este primer éxito, las tecnologías inventadas, perfeccionadas, implementadas y sustituidas por otras mejores, permitieron un progreso asombroso: hoy es posible descodificar un genoma humano en su totalidad por dos mil dólares y en el plazo de un par de semanas. Pero el avance tampoco acaba aquí, y es posible prever la disponibilidad en los próximos diez años de tecnologías que permitirán la descodificación de un genoma por menos de diez centavos y en solo un segundo. Cabe pensar en las transformaciones que comportará este tipo de cambio en el ámbito de la sanidad, los seguros, la privacidad y muchos otros más.
El mensaje es que no hay nada mágico en un determinado límite al que acabamos llamando unidad o 100 %, y que fue el poder del invento y de la implementación el que hizo que las tecnologías alcanzaran este umbral no se detuvieran sino que continuaran, ofreciendo mejoras en el resultado deseado a precios más bajos y a una mayor velocidad.</p>

        <div style="margin-top: 50px; padding-top: 30px; border-top: 1px solid var(--border-color); text-align: right;">
            <a href="04-inteligencia-artificial.html" style="color: var(--primary-color); font-weight: 600; text-decoration: none;">
                Next: Chapter 4: Inteligencia artificial →
            </a>
        </div>
        </div>

        <div class="footer">
            <p>&copy; 2018 David Orban</p>
            <p><a href="https://davidorban.com">davidorban.com</a></p>
        </div>
    </div>
</body>
</html>